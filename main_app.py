import re
import threading
import os
from dotenv import load_dotenv  # Only needed for local development
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# Load environment variables from a .env file (for local development)
load_dotenv()

# Retrieve the OpenAI API key from the environment variables
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("The OPENAI_API_KEY environment variable is not set!")

# Global objects to be initialized later
vectorstore = None
qa_chain = None
llm_flowchart = None

def create_qa_chain(vectorstore):
    """
    Create a RetrievalQA chain using a ChatOpenAI model for generating answers.
    """
    llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", 
        retriever=vectorstore.as_retriever()
    )

def clean_mermaid_code(mermaid_code: str) -> str:
    """
    Remove triple backticks and any 'mermaid' fences that might wrap the code.
    """
    code = mermaid_code.strip()
    code = re.sub(r"^```mermaid\s*", "", code, flags=re.IGNORECASE)
    code = re.sub(r"^```", "", code, flags=re.IGNORECASE)
    code = re.sub(r"```$", "", code, flags=re.IGNORECASE)
    return code.strip()

def render_mermaid(mermaid_code: str) -> str:
    """
    Return an HTML snippet that embeds the Mermaid code in a <div class="mermaid">.
    """
    return f"""
    <div class="mermaid">
    {mermaid_code}
    </div>
    <script>
      if (typeof mermaid !== "undefined") {{
         mermaid.init(undefined, document.getElementsByClassName("mermaid"));
      }}
    </script>
    """

# Pydantic models for request validation
class ChatRequest(BaseModel):
    question: str

class FlowchartRequest(BaseModel):
    description: str

# Initialize FastAPI app
app = FastAPI()

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Receives a question and returns an answer generated by the QA chain.
    """
    try:
        answer = qa_chain.run(request.question)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"answer": answer}

@app.post("/flowchart")
async def flowchart_endpoint(request: FlowchartRequest):
    """
    Receives a description and returns a Mermaid flowchart generated based on retrieved context.
    """
    try:
        # Retrieve relevant documents based on the description
        docs = vectorstore.as_retriever().invoke(request.description)
        context = "\n".join([doc.page_content for doc in docs])
        prompt = (
            "Based on the following context extracted from documents:\n"
            f"{context}\n\n"
            "Generate a flowchart in Mermaid syntax that outlines the process for: "
            f"{request.description}\n"
            "Only return the raw Mermaid code."
            "The code should start with 'graph' and include all necessary nodes and edges. Text inside the square brackets should be wrapped up in the quootation marks, it is essential."
            "Try to generate tree-like node, if its possible."
            "Below is the example of the right version:"
            "graph LR:"
            "A   ['Начало'] --> B['Приказ Министра науки и высшего образования РК от 03.05.2024 № 212']"
            "B --> C[        'Приложение 2-2']"
            "C --> D[    'Шкала перевода баллов сертификата КАЗТЕСТ в баллы ЕНТ']"
        )
        result = llm_flowchart.invoke([HumanMessage(content=prompt)])
        flowchart_code_cleaned = clean_mermaid_code(result.content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"flowchart": flowchart_code_cleaned}

def run_fastapi():
    """
    Start the FastAPI server.
    """
    uvicorn.run(app, host="0.0.0.0", port=8000)

def main():
    global vectorstore, qa_chain, llm_flowchart

    # Load the FAISS index from disk
    index_path = "faiss_index"
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    try:
        vectorstore = FAISS.load_local(
            index_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print(f"Loaded FAISS index from '{index_path}'.")
    except Exception as e:
        print("Error loading FAISS index:", e)
        return

    # Initialize the QA chain and the LLM for flowchart generation
    qa_chain = create_qa_chain(vectorstore)
    llm_flowchart = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)

    # Run the FastAPI server
    run_fastapi()

if __name__ == '__main__':
    main()
